# -*- coding: utf-8 -*-
"""Advanced visualizations and Predictions with LSTM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/hecongqing/advanced-visualizations-and-predictions-with-lstm.e9e25ceb-48d6-4528-8626-8adccb5302fa.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250721/auto/storage/goog4_request%26X-Goog-Date%3D20250721T143324Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8b2ee562a7fe7823fbbe311c893c2162da6c5db34cc344346b2ee43fa530e7cdf0665fe73b20357537b9d108d826b448e1b04a245ae0b4aa6dcc977c83bec7a99db1eaaefd7d5601dfd20822b986448acf8c2d35f419c252fa02c7dc3c46ed7afd96dec140fac914e4368f328deb1d8b42841784b28272a9e24856736bb8d70a70c1153b1bc4bd2373f766f758d0d8e3ae8a1447ce3c6cd0e3ab0f5a1bfbab75135e5b4f691036e8a15732222b501f2901c64579a6836db02f1dc327dafc1ebc0779692151d143552d8cec7a02923be1bd1c772289b187d9007c5822fe38a732626aa605659400775dd303d70c7c7139be442d80b66ebfd0b34170eb68eeb680
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
hmavrodiev_london_bike_sharing_dataset_path = kagglehub.dataset_download('hmavrodiev/london-bike-sharing-dataset')

print('Data source import complete.')

"""#  **Bike Demand Prediction with LSTMs using TensorFlow and Keras **

![image.png](attachment:image.png)

# **Bidirectional LSTM :**
Using Bidirectional LSTMs, we feed the learning algorithm with the original data once from beginning to the end and once from end to beginning.

![image.png](attachment:image.png)

# **Goal:**
* ***Our goal is to predict the number of future bike shares given the historical data of London bike shares.***

# **Here are the steps:**
* <a href='#Data'>Data</a>
* <a href='#Feature Engineering'>Feature Engineering</a>
* <a href='#Exploratory Data Analysis'>Exploratory Data Analysis</a>
* <a href='#Preprocessing'>Preprocessing</a>
* <a href='#Model Building'>Model Building</a>
* <a href='#Demand Predictions'>Demand Predictions</a>
* <a href='#Evaluation'>Evaluation</a>

<a id='Data'></a>
# **Data**

Metadata:
* "timestamp" - timestamp field for grouping the data
* "cnt" - the count of a new bike shares
* "t1" - real temperature in C
* "t2" - temperature in C "feels like"
* "hum" - humidity in percentage
* "windspeed" - wind speed in km/h
* "weathercode" - category of the weather
* "isholiday" - boolean field - 1 holiday / 0 non holiday
* "isweekend" - boolean field - 1 if the day is weekend
* "season" - category field meteorological seasons: 0-spring ; 1-summer; 2-fall; 3-winter
* "weathe_code" category description:
* 1 = Clear ; mostly clear but have some values with haze/fog/patches of fog/ fog in vicinity 2 = scattered clouds / few clouds 3 = Broken clouds 4 = Cloudy 7 = Rain/ light Rain shower/ Light rain 10 = rain with thunderstorm 26 = snowfall 94 = Freezing Fog
"""

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
data = pd.read_csv("../input/london-bike-sharing-dataset/london_merged.csv")

data.head()

"""# **GPU**"""

import tensorflow.compat.v1 as tf
print(tf.test.gpu_device_name())
# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth
config = tf.ConfigProto()
config.gpu_options.allow_growth = True

"""<a id='Feature Engineering'></a>
# **Feature Engineering**
"""

import pandas as pd
import numpy as np

data.shape
data.info(memory_usage="deep")

import seaborn as sns
import matplotlib.pyplot as plt
sns.set(context="notebook", style="darkgrid", palette="deep", font="sans-serif", font_scale=1, color_codes=True)

data.columns
data.head()

data["timestamp"] = pd.to_datetime(data["timestamp"])

data = data.set_index("timestamp")

data["hour"] = data.index.hour
data["day_of_month"] = data.index.day
data["day_of_week"]  = data.index.dayofweek
data["month"] = data.index.month

data.columns
data.shape

corr_matrix = data.corr().abs()
high_corr_var=np.where(corr_matrix>0.8)
high_corr_var=[(corr_matrix.columns[x],corr_matrix.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]

print(high_corr_var)

data.head()

"""<a id='Exploratory Data Analysis'></a>
# **Exploratory Data Analysis**
"""

plt.figure(figsize=(16,6))
sns.heatmap(data.corr(),cmap="YlGnBu",square=True,linewidths=.5,center=0,linecolor="red")

"""From the above heatmap we can say that **t1** and **t2** are Highly Correlated with each other."""

plt.figure(figsize=(16,6))
data.isnull().sum()
sns.heatmap(data.isnull(),cmap="viridis")

"""**This clean heatmap says that data does not have even a single Null value.**"""

plt.figure(figsize=(15,6))
sns.lineplot(data=data,x=data.index,y=data.cnt)
plt.xticks(rotation=90)

df_by_month = data.resample("M").sum()

plt.figure(figsize=(16,6))
sns.lineplot(data=df_by_month,x=df_by_month.index,y=df_by_month.cnt,color="red")
plt.xticks(rotation=90)

plt.figure(figsize=(16,6))
sns.pointplot(data=data,x=data.hour,y=data.cnt,color="black")

"""**Here we can say that between morning 8 to 9 the Demand is on peak.**"""

plt.figure(figsize=(16,6))
sns.pointplot(data=data,x=data.month,y=data.cnt,color="red")

"""**By looking at the above graph we can say that in the July month the demand is on peak and then start to decrease.**"""

plt.figure(figsize=(16,6))
sns.pointplot(data=data,x=data.day_of_week,y=data.cnt,color="black")

"""**Least demand comes from weekend.**"""

plt.figure(figsize=(16,6))
sns.lineplot(data=data,x=data.day_of_month,y=data.cnt,color="r")

plt.figure(figsize=(16,6))
sns.pointplot(data=data,x=data.hour,y=data.cnt,hue=data.is_holiday)

plt.figure(figsize=(16,6))
sns.pointplot(data=data,x=data.month,y=data.cnt,hue=data.is_holiday)

plt.figure(figsize=(16,6))
sns.pointplot(data=data,hue=data.season,y=data.cnt,x=data.month)

plt.figure(figsize=(16,6))
sns.countplot(data=data,hue=data.is_holiday,x=data.season)

plt.figure(figsize=(16,6))
sns.pointplot(data=data,x=data.hum,y=data.cnt,color="black")
plt.xticks(rotation=90)

"""**Demand decreases with Increasing humidity.**"""

plt.figure(figsize=(16,6))
sns.pointplot(data=data,x=data.wind_speed,y=data.cnt)
plt.xticks(rotation=90)

plt.figure(figsize=(16,6))
sns.pointplot(data=data,x=data.weather_code,y=data.cnt)
plt.xticks(rotation=90)

"""**Maximul demands comes in scattered coluds weather.**"""

plt.figure(figsize=(16,6))
sns.lineplot(x=data.hour,y=data.cnt,data=data,hue=data.is_weekend)

plt.figure(figsize=(16,6))
sns.pointplot(x=data.hour,y=data.cnt,data=data,hue=data.season)

plt.figure(figsize=(16,6))
sns.pointplot(x=data.hour,y=data.cnt,data=data,hue=data.weather_code)

plt.figure(figsize=(16,6))
sns.countplot(data=data,x=data.day_of_week,hue=data.weather_code,palette="viridis")
plt.legend(loc="best")

plt.figure(figsize=(16,6))
sns.boxplot(data=data,x=data["hour"],y=data.cnt)

plt.figure(figsize=(16,6))
sns.boxplot(data=data,x=data["day_of_week"],y=data.cnt)

plt.figure(figsize=(16,6))
sns.boxplot(data=data,x=data["day_of_month"],y=data.cnt)

plt.figure(figsize=(16,6))
sns.boxplot(data=data,x=data["month"],y=data.cnt)

plt.figure(figsize=(16,6))
sns.boxplot(data=data,x=data["day_of_month"],y=data.cnt,hue=data["is_holiday"])

"""<a id='Preprocessing'></a>
# **Preprocessing**

* Training set = 90% of tha data
* Testing set  = 10% of the data
"""

from sklearn.model_selection import train_test_split
train,test = train_test_split(data,test_size=0.1,random_state=0)

print(train.shape)
print(test.shape)

from sklearn.preprocessing import MinMaxScaler
scaler  = MinMaxScaler()

num_colu = ['t1', 't2', 'hum', 'wind_speed']
trans_1 = scaler.fit(train[num_colu].to_numpy())
train.loc[:,num_colu] = trans_1.transform(train[num_colu].to_numpy())
test.loc[:,num_colu] = trans_1.transform(test[num_colu].to_numpy())

cnt_scaler = MinMaxScaler()
trans_2 = cnt_scaler.fit(train[["cnt"]])
train["cnt"] = trans_2.transform(train[["cnt"]])
test["cnt"] = trans_2.transform(test[["cnt"]])

from tqdm import tqdm_notebook as tqdm
tqdm().pandas()
def prepare_data(X,y,time_steps=1):
    Xs = []
    Ys = []
    for i in tqdm(range(len(X) - time_steps)):
        a = X.iloc[i:(i + time_steps)].to_numpy()
        Xs.append(a)
        Ys.append(y.iloc[i+time_steps])
    return np.array(Xs),np.array(Ys)

steps=24
X_train , y_train = prepare_data(train,train.cnt,time_steps=steps)
X_test , y_test = prepare_data(test,test.cnt,time_steps=steps)
print("X_train : {}\nX_test : {}\ny_train : {}\ny_test: {}".format(X_train.shape,X_test.shape,y_train.shape,y_test.shape))

"""<a id='Model Building'></a>
# **Model Building**
"""

### LSTMM model
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout , LSTM , Bidirectional


model = Sequential()
model.add(Bidirectional(LSTM(128,input_shape=(X_train.shape[1],X_train.shape[2]))))
model.add(Dropout(0.2))
model.add(Dense(1,activation="sigmoid"))
model.compile(optimizer="adam",loss="mse")

with tf.device('/GPU:0'):
    prepared_model = model.fit(X_train,y_train,batch_size=32,epochs=100,validation_data=[X_test,y_test])

plt.plot(prepared_model.history["loss"],label="loss")
plt.plot(prepared_model.history["val_loss"],label="val_loss")
plt.legend(loc="best")
plt.xlabel("No. Of Epochs")
plt.ylabel("mse score")

"""<a id='Demand Predictions'></a>
# **Demand Predictions**
"""

pred = model.predict(X_test)

y_test_inv = cnt_scaler.inverse_transform(y_test.reshape(-1,1))
pred_inv = cnt_scaler.inverse_transform(pred)

plt.figure(figsize=(16,6))
plt.plot(y_test_inv.flatten(),marker=".",label="actual")
plt.plot(pred_inv.flatten(),marker=".",label="prediction",color="r")

"""**Note that our model is predicting only one point in the future. That being said, it is doing very well. Although our model can’t really capture the extreme values it does a good job of predicting (understanding) the general pattern.**"""

y_test_actual = cnt_scaler.inverse_transform(y_test.reshape(-1,1))
y_test_pred = cnt_scaler.inverse_transform(pred)

arr_1 = np.array(y_test_actual)
arr_2 = np.array(y_test_pred)

actual = pd.DataFrame(data=arr_1.flatten(),columns=["actual"])
predicted = pd.DataFrame(data=arr_2.flatten(),columns = ["predicted"])

final = pd.concat([actual,predicted],axis=1)
final.head()

"""<a id='Evaluation'></a>
# **Evaluation**
"""

from sklearn.metrics import mean_squared_error, r2_score
rmse = np.sqrt(mean_squared_error(final.actual,final.predicted))
r2 = r2_score(final.actual,final.predicted)
print("rmse is : {}\nr2 is : {}".format(rmse,r2))

plt.figure(figsize=(16,6))
plt.plot(final.actual,label="Actual data")
plt.plot(final.predicted,label="predicted values")
plt.legend(loc="best")

"""# Conclusion :
**AS you can see that I have used Bidirectional LSTM to train our model and Our model is working quite well.Our model is capable to capture the trend and not capturing the Extreme values which is a really good thing. SO, we can say that the overall perfomance is good.**

# If you like this notebook Please Do give an Upvote.

[](http://)
"""